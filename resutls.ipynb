{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = pd.read_csv('returns.csv', header=None)\n",
    "# df = df.apply(lambda iterator: ((iterator - iterator.mean())/iterator.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021732</td>\n",
       "      <td>0.023444</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>-0.003098</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>0.017476</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.063718</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013942</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.017371</td>\n",
       "      <td>0.007844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005219</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>-0.009322</td>\n",
       "      <td>0.017449</td>\n",
       "      <td>0.018053</td>\n",
       "      <td>-0.003690</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>-0.022159</td>\n",
       "      <td>-0.020699</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>-0.024625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>-0.003115</td>\n",
       "      <td>-0.008234</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020796</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>0.008644</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.006550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000808</td>\n",
       "      <td>-0.008324</td>\n",
       "      <td>0.029286</td>\n",
       "      <td>0.024115</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>-0.004292</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>-0.009681</td>\n",
       "      <td>-0.004138</td>\n",
       "      <td>-0.025046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>-0.025285</td>\n",
       "      <td>-0.006735</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>-0.008246</td>\n",
       "      <td>-0.010368</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>-0.008992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007161</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.007634</td>\n",
       "      <td>0.015634</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>-0.013158</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017847</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.002558</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>-0.002746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>-0.005340</td>\n",
       "      <td>-0.001371</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>-0.007024</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>-0.002977</td>\n",
       "      <td>-0.005046</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.018258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>-0.011201</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>-0.007551</td>\n",
       "      <td>-0.006131</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>-0.008107</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>-0.001920</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>-0.011531</td>\n",
       "      <td>-0.006203</td>\n",
       "      <td>0.009972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.027799</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>0.017163</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.019188</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.018220</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.007052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>-0.013059</td>\n",
       "      <td>-0.007061</td>\n",
       "      <td>-0.004244</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>-0.007353</td>\n",
       "      <td>-0.004610</td>\n",
       "      <td>-0.020232</td>\n",
       "      <td>-0.013310</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009237</td>\n",
       "      <td>-0.004295</td>\n",
       "      <td>-0.014767</td>\n",
       "      <td>-0.009320</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>-0.003203</td>\n",
       "      <td>-0.005246</td>\n",
       "      <td>-0.007226</td>\n",
       "      <td>-0.011614</td>\n",
       "      <td>-0.015717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>-0.019195</td>\n",
       "      <td>-0.019127</td>\n",
       "      <td>-0.014740</td>\n",
       "      <td>-0.008125</td>\n",
       "      <td>-0.014351</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>-0.019964</td>\n",
       "      <td>-0.012903</td>\n",
       "      <td>-0.013120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008431</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>-0.007623</td>\n",
       "      <td>-0.014000</td>\n",
       "      <td>-0.014722</td>\n",
       "      <td>-0.010628</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.011858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.021732  0.023444  0.002508 -0.003098  0.009709 -0.000547  0.017476   \n",
       "1     0.005219  0.004288  0.003842 -0.009322  0.017449  0.018053 -0.003690   \n",
       "2     0.008180  0.013026 -0.003115 -0.008234  0.006754  0.001612  0.011543   \n",
       "3    -0.000808 -0.008324  0.029286  0.024115  0.003865 -0.004292  0.007273   \n",
       "4     0.007161 -0.001991 -0.007634  0.015634  0.008801 -0.007004  0.004792   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1253 -0.005340 -0.001371 -0.002687  0.002125  0.001843 -0.003877 -0.003474   \n",
       "1254 -0.011201  0.018724  0.005030 -0.007551 -0.006131  0.002271  0.021414   \n",
       "1255  0.017974  0.027799  0.010724  0.018162  0.012647  0.012298  0.014983   \n",
       "1256 -0.013059 -0.007061 -0.004244  0.001812 -0.002437 -0.007353 -0.004610   \n",
       "1257 -0.019195 -0.019127 -0.014740 -0.008125 -0.014351  0.003221 -0.015551   \n",
       "\n",
       "            7         8         9   ...        40        41        42  \\\n",
       "0     0.015424  0.063718  0.015817  ...  0.013942  0.002237  0.022508   \n",
       "1     0.016591  0.003524 -0.001342  ...  0.015743 -0.011905  0.003241   \n",
       "2    -0.009042  0.018258  0.012455  ...  0.020796  0.004217  0.013256   \n",
       "3    -0.009681 -0.004138 -0.025046  ...  0.012301  0.008698  0.000836   \n",
       "4     0.007360 -0.013158 -0.003450  ... -0.017847  0.001784 -0.002702   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1253 -0.007024 -0.004037 -0.002903  ...  0.010400  0.006996  0.002274   \n",
       "1254 -0.001875 -0.008107 -0.000336  ...  0.000606  0.005127 -0.001920   \n",
       "1255  0.017163  0.008757  0.006497  ...  0.026220  0.019188  0.006470   \n",
       "1256 -0.020232 -0.013310 -0.007457  ... -0.009237 -0.004295 -0.014767   \n",
       "1257 -0.019964 -0.012903 -0.013120  ... -0.008431 -0.006454 -0.004937   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "0     0.018167  0.014880 -0.008362  0.005078  0.005232  0.017371  0.007844  \n",
       "1     0.020038 -0.022159 -0.020699 -0.002210 -0.004673  0.005155 -0.024625  \n",
       "2     0.013724  0.011927 -0.005950 -0.002215  0.008644  0.009615  0.006550  \n",
       "3    -0.025285 -0.006735 -0.007245 -0.008246 -0.010368  0.018730 -0.008992  \n",
       "4    -0.008280  0.002882 -0.001745 -0.002558  0.002031  0.002181 -0.002746  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1253 -0.005112  0.000573 -0.002977 -0.005046 -0.007297 -0.004043 -0.018258  \n",
       "1254  0.000856 -0.024889 -0.001148 -0.006174 -0.011531 -0.006203  0.009972  \n",
       "1255  0.009840  0.018220  0.005059  0.018860  0.008895  0.011565  0.007052  \n",
       "1256 -0.009320 -0.004865 -0.003203 -0.005246 -0.007226 -0.011614 -0.015717  \n",
       "1257 -0.011760 -0.007623 -0.014000 -0.014722 -0.010628 -0.007436 -0.011858  \n",
       "\n",
       "[1258 rows x 50 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai\n",
    "def sample_cov(df):\n",
    "    cov = np.array(df - df.mean(axis=0))\n",
    "    total = np.zeros((cov.shape[1], cov.shape[1]))\n",
    "    for r in range(cov.shape[0]):\n",
    "        total += cov[[r], :].T @ cov[[r], :]\n",
    "    return total/cov.shape[0]\n",
    "\n",
    "\n",
    "sample_covariance = sample_cov(df)\n",
    "np.savetxt('sample_covariance.csv', sample_covariance, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(S, lam=0.00001, iterations = 10000, stopping_thres = 0):\n",
    "    T, N = df.shape\n",
    "    J = np.eye(N)\n",
    "    def assign(i,j, new):\n",
    "        J[i][i] = (new)[0][0]\n",
    "        J[i][j] = (new)[0][1]\n",
    "        J[j][i] = (new)[1][0]\n",
    "        J[j][j] = (new)[1][1]\n",
    "    def obj():\n",
    "        return np.log(np.linalg.det(J))-np.trace(J @ S) - lam * np.sum(np.abs(J - np.diag(np.diag(J))))\n",
    "        # return np.log(np.linalg.det(J))-np.trace(J @ S) - lam * (sum(sum(abs(J))) - np.trace(abs(J)))\n",
    "    prev_obj = obj()\n",
    "    for _ in range(iterations):\n",
    "        for i in range(N):\n",
    "            for j in range(i+1,N):\n",
    "                Jbb = J[[a for a in range(N) if not a in [i,j]], :][:, [a for a in range(N) if not a in [i,j]]]\n",
    "                Jab = J[[i, j], :][:, [a for a in range(N) if not a in [i,j]]]\n",
    "                Jba = J[[a for a in range(N) if not a in [i,j]], :][:, [i, j]]\n",
    "                L = Jab@np.linalg.inv(Jbb)@Jba\n",
    "                Saa = S[[i, j], :][:, [i, j]]\n",
    "                Svv = S[j][j]\n",
    "                Suu = S[i][i]\n",
    "                ms = []\n",
    "                try: \n",
    "                    m1 = np.linalg.inv(Saa+lam*np.array([[0, 1], [1, 0]]))\n",
    "                except: \n",
    "                    pass\n",
    "                else:\n",
    "                    if np.all(np.linalg.eigvals(m1) > 0):\n",
    "                        ms.append(m1)\n",
    "                try: \n",
    "                    m2 = np.linalg.inv(Saa+lam*np.array([[0, -1], [-1, 0]]))\n",
    "                except: \n",
    "                    pass\n",
    "                else:\n",
    "                    if np.all(np.linalg.eigvals(m2) > 0):\n",
    "                        ms.append(m2)\n",
    "                try: \n",
    "                    m3 = np.array([[(1+np.sqrt(1+4*Svv*Suu*L[0][1]**2))/(2*Suu), -L[0][1]],\n",
    "                                [-L[1][0], (1+np.sqrt(1+4*Svv*Suu*L[0][1]**2))/(2*Svv)]])\n",
    "                except: \n",
    "                    pass\n",
    "                else:\n",
    "                    if np.all(np.linalg.eigvals(m3) > 0) and np.any(np.isnan(m3)):\n",
    "                        ms.append(m3)\n",
    "                curr_obj = obj()\n",
    "                for m in ms:\n",
    "                    prev_Jaa = J[[i,j],:][:,[i,j]]\n",
    "                    assign(i, j, m+L)\n",
    "                    new_obj = obj()\n",
    "                    if new_obj>curr_obj:\n",
    "                        curr_obj = new_obj\n",
    "                    else:\n",
    "                        assign(i, j, prev_Jaa)\n",
    "        # if obj()-prev_obj < stopping_thres:\n",
    "            # break\n",
    "        if curr_obj-prev_obj > stopping_thres:\n",
    "            prev_obj = curr_obj\n",
    "            print(curr_obj)\n",
    "        else:\n",
    "            break\n",
    "    print(np.count_nonzero(J))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385.0056759997497\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "J_test = run(sample_covariance, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380.4197530017843\n",
      "391.0326624128037\n",
      "394.1786065074648\n",
      "395.3742950060432\n",
      "395.88432341540766\n",
      "396.26606417195796\n",
      "396.6505194282163\n",
      "397.01563621991335\n",
      "397.3463916676511\n",
      "397.6223324398686\n",
      "397.86358348742414\n",
      "398.0631110785621\n",
      "398.2164931587084\n",
      "398.32723163974885\n",
      "398.41790228705094\n",
      "398.4693825025649\n",
      "398.51013741665287\n",
      "398.5349459835055\n",
      "398.5573109614745\n",
      "398.57676925066045\n",
      "398.5851492784454\n",
      "398.591094718401\n",
      "398.59511073704283\n",
      "398.5987347747252\n",
      "398.60207692292\n",
      "398.604096897465\n",
      "398.60541623694223\n",
      "398.606222102745\n",
      "398.60674791121056\n",
      "398.6071651829852\n",
      "398.60760241591043\n",
      "398.6079547510949\n",
      "398.60817726214196\n",
      "398.60839552713975\n",
      "398.6085543237606\n",
      "398.6086528369176\n",
      "398.60873079749734\n",
      "398.60879243628966\n",
      "398.60884212886276\n",
      "398.6088812176039\n",
      "398.60891292230065\n",
      "398.6089450873016\n",
      "398.6089661444202\n",
      "398.60898291851254\n",
      "398.6089963185406\n",
      "398.6090070463206\n",
      "398.6090155408171\n",
      "398.60902230804885\n",
      "398.6090277133595\n",
      "398.60903206409665\n",
      "398.60903559628537\n",
      "398.60903848816525\n",
      "398.6090408547387\n",
      "398.6090427977645\n",
      "398.6090443947237\n",
      "398.6090456933374\n",
      "398.6090467438747\n",
      "398.60904758836637\n",
      "398.6090482635423\n",
      "398.60904880991666\n",
      "398.60904923933566\n",
      "398.60904958176496\n",
      "398.609049856884\n",
      "398.60905008004966\n",
      "398.6090502626213\n",
      "398.60905041275157\n",
      "398.60905053629415\n",
      "398.60905063762607\n",
      "398.6090507202135\n",
      "398.609050787015\n",
      "398.60905084065774\n",
      "398.60905088351655\n",
      "398.60905091769496\n",
      "398.6090509450063\n",
      "398.60905096694626\n",
      "398.6090509847027\n",
      "398.6090509991833\n",
      "398.6090510110684\n",
      "398.609051020847\n",
      "398.6090510288829\n",
      "398.6090510354554\n",
      "398.60905104079853\n",
      "398.6090510451056\n",
      "398.6090510485548\n",
      "398.6090510513033\n",
      "398.6090510534998\n",
      "398.6090510552583\n",
      "398.6090510566734\n",
      "398.60905105782166\n",
      "398.6090510587603\n",
      "398.609051059535\n",
      "398.6090510601742\n",
      "398.6090510606983\n",
      "398.60905106112887\n",
      "398.60905106147663\n",
      "398.609051061759\n",
      "398.6090510619813\n",
      "398.609051062159\n",
      "398.6090510623007\n",
      "398.6090510624148\n",
      "398.60905106250715\n",
      "398.609051062584\n",
      "398.6090510626488\n",
      "398.6090510627016\n",
      "398.60905106274447\n",
      "398.6090510627764\n",
      "398.6090510628027\n",
      "398.60905106282223\n",
      "398.60905106283514\n",
      "398.6090510628445\n",
      "398.60905106284986\n",
      "398.6090510628533\n",
      "398.6090510628556\n",
      "1996\n"
     ]
    }
   ],
   "source": [
    "new_J = run(sample_covariance, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362.3997860189677\n",
      "363.68106019009815\n",
      "363.9072771542286\n",
      "364.0166969285136\n",
      "364.1188795927008\n",
      "364.14068152791117\n",
      "364.1510739248299\n",
      "364.1580863216129\n",
      "364.1636093685146\n",
      "364.1661778994537\n",
      "364.16755387925906\n",
      "364.16791826797123\n",
      "364.1679632315418\n",
      "364.1679798737139\n",
      "364.1679910978421\n",
      "364.16800018817395\n",
      "364.1680027557484\n",
      "364.16800312650344\n",
      "364.1680032746521\n",
      "364.16800334400364\n",
      "364.1680033804277\n",
      "364.168003399888\n",
      "364.16800340978716\n",
      "364.16800341446185\n",
      "364.1680034165032\n",
      "364.16800341733875\n",
      "364.1680034176731\n",
      "364.1680034178134\n",
      "364.16800341787626\n",
      "364.16800341790804\n",
      "364.1680034179245\n",
      "364.16800341793146\n",
      "364.1680034179356\n",
      "364.1680034179369\n",
      "364.1680034179376\n",
      "364.16800341793817\n",
      "364.1680034179389\n",
      "778\n"
     ]
    }
   ],
   "source": [
    "new_J1 = run(sample_covariance, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "new_J2 = run(sample_covariance, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\n",
      "778\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(new_J))\n",
    "print(np.count_nonzero(new_J1))\n",
    "print(np.count_nonzero(new_J2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2500\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.linalg.inv(new_J)))\n",
    "print(np.count_nonzero(np.linalg.inv(new_J1)))\n",
    "print(np.count_nonzero(np.linalg.inv(new_J2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "2500\n",
      "538\n",
      "1942\n",
      "50\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.522957876285094e-11, tolerance: 4.448338758712089e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0171922562200253e-09, tolerance: 3.1977992399623245e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.561371455076433e-09, tolerance: 5.610078280262789e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.97141714045951e-10, tolerance: 8.150357511094228e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.014398755634082e-10, tolerance: 1.0621407568496749e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8475176247965903e-10, tolerance: 9.700442299789227e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.079849456516813e-10, tolerance: 2.1373318707067757e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.411657797837856e-10, tolerance: 3.7396725099191535e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8668998804711604e-10, tolerance: 1.5842861079855182e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2806573843678736e-09, tolerance: 3.1013443331290154e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.1013130581578e-11, tolerance: 3.132810978919689e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.5703209487492286e-11, tolerance: 5.145816846523071e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6897125699270855e-08, tolerance: 5.060951510756825e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.378009630651904e-10, tolerance: 1.0520914187306854e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.970171602652009e-09, tolerance: 5.2680523319608085e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.663988791161683e-09, tolerance: 6.38425944694407e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0698270240072465e-10, tolerance: 5.0698120408162306e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.191399809035809e-09, tolerance: 4.920655465435694e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8903543758542335e-10, tolerance: 2.199765281169151e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.308263831757629e-11, tolerance: 4.0674528592349604e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.107604583355854e-11, tolerance: 4.448338758712089e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.521814273447268e-10, tolerance: 3.1977992399623245e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7717836787973324e-09, tolerance: 5.610078280262789e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.799563517958333e-10, tolerance: 5.5372477947513575e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.697399869262486e-10, tolerance: 8.150357511094228e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8495280614927011e-09, tolerance: 1.0621407568496749e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1891235536548176e-10, tolerance: 3.7396725099191535e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.664801066310651e-11, tolerance: 3.1013443331290154e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0052828712992113e-09, tolerance: 5.060951510756825e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1184409816746097e-09, tolerance: 1.0520914187306854e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.003239955714273e-10, tolerance: 5.2680523319608085e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.396959241333929e-10, tolerance: 6.38425944694407e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.314374956328795e-10, tolerance: 4.920655465435694e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6606470963571917e-09, tolerance: 5.610078280262789e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.7602172489311376e-11, tolerance: 5.5372477947513575e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.781774581935229e-10, tolerance: 8.150357511094228e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.165301215850725e-10, tolerance: 1.0621407568496749e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0797879453251893e-10, tolerance: 5.060951510756825e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8241952981262917e-10, tolerance: 1.0520914187306854e-10\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/shuge/opt/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:229: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2500655513595612e-10, tolerance: 5.610078280262789e-11\n",
      "  coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import graphical_lasso\n",
    "# sample_covariance = df.cov().to_numpy()\n",
    "S, J = graphical_lasso(sample_covariance, 0.00001)\n",
    "print(np.count_nonzero(J)) # 1132\n",
    "print(np.count_nonzero(S)) # 2500\n",
    "S1, J1 = graphical_lasso(sample_covariance, 0.0001)\n",
    "print(np.count_nonzero(J1)) # 538\n",
    "print(np.count_nonzero(S1)) # 1942\n",
    "S2, J2 = graphical_lasso(sample_covariance, 0.001)\n",
    "print(np.count_nonzero(J2)) # 50\n",
    "print(np.count_nonzero(S2)) # 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a6df60bedf1c0dd760e71efb82970e84ef35f476934fd5ef0683dc3fe4c3487"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
